Overview
########

Josiann is a Python library which implements the simulated annealing algorithm for noisy cost functions.
It has support for vectorized functions, multiprocessing and provides a parallel mode for optimizing several similar
but independent problems at once.

Description
***********

A classical problem in optimization consists in finding the global minimum of some cost function.
In several cases however, measures of the cost function might carry noise because of imprecise measurement techniques
or because of intrinsic randomness.
Josiann is an implementation of the modified Simulated Annealing algorithm introduced in [1]_.

.. warning::

    The algorithm converges under the condition that the cost function has the form :math:`f(x) = g(x) + e(x)`, where
    :math:`e(x)` is a random term drawn from a random symmetrical distribution with mean 0.

Briefly, the algorithm takes multiple evaluations of the cost function and averages them together to reduce the amount
of noise.
The number of evaluations to be averaged depends on the Temperature parameter and on the maximum allowed number of
evaluations, but grows with the number of iterations (so that the standard error of the noise decreases on the order
of :math:`O(k^{-\gamma})` with k the iteration number and :math:`\gamma` an arbitrary constant larger than 1).


Implementations
***************

Josiann provides 4 implementations of the algorithm, 3 sequential modes and 1 parallel mode.

Sequential modes
================

In sequential modes, **ONE** problem is solved at a time though the cost function can be any dimensional.

- :func:`josiann.sa` is the base version of the algorithm.
- :func:`josiann.vsa` expects a vectorized cost function to compute multiple evaluations of the function at once (it
  either evaluates the cost function multiple times at the same position or evaluates it at different positions when
  possible).
- :func:`josiann.mcsa` expects a regular cost function but runs on multiple CPU cores to obtain multiple evaluations
  faster.

.. note::

    - :func:`josiann.sa` and :func:`josiann.mcsa` expect a cost function of the form `f: n-dim vector ⟶ float`.
    - :func:`josiann.vsa` expects a function of the form `f: (n, m) matrix ⟶ m-dim vector`.

    where m is the number of evaluations to compute.

Parallel mode
=============

In parallel mode, multiple independent optimization problems are solved in parallel.
The cost functions can be any-dimensional but must be the same in each parallel optimization task.

- :func:`josiann.parallel.psa` is the parallel version of the algorithm.

.. note::

    As for :func:`josiann.vsa`, :func:`josiann.parallel.psa` expects a cost functions of the form
    `f: (n, m) matrix ⟶ m-dim vector`.

More precisely, the cost function should take as first argument a :class:`josiann.parallel.ParallelArgument` object
which will hold all instructions for one iteration's computation :

- :attr:`josiann.parallel.ParallelArgument.positions` : the position vectors (in an (n, m) matrix)
- :attr:`josiann.parallel.ParallelArgument.nb_evaluations` : how many evaluations are required per position vector

The functions evaluations should not be returned by the cost function but rather store in the
:attr:`josiann.parallel.ParallelArgument.result` attribute which will ensure that all function evaluations were
indeed computed.


Moves
*****

*Moves* define how a position vector (at which the cost function is evaluated) is modified at each
iteration to generate new candidate positions.
Some commonly used moves have been implemented :

Sequential moves :mod:`josiann.moves.sequential`
================================================

- :class:`josiann.moves.sequential.Metropolis` (next position is drawn at random from a multivariate normal
  distribution centered at the current position)
- :class:`josiann.moves.sequential.Metropolis1D` (same as Metropolis but only one dimension is updated per iteration.)
- :class:`josiann.moves.sequential.RandomStep` (next position is generated by incrementing one coordinate of the
  current position vector by a random value. The updated coordinate is chosen at random.)

Ensemble moves :mod:`josiann.moves.ensemble` (moves with multiple :ref:`walkers <walkers>`)
===========================================================================================

- :class:`josiann.moves.ensemble.Stretch` (adapted from [2]_)
- :class:`josiann.moves.ensemble.StretchAdaptive` (adapted from [2]_ with varying parameter a.)

Discrete moves :mod:`josiann.moves.discrete`
============================================

- :class:`josiann.moves.discrete.SetStep` (A set of possible positions :math:`{a_1, ..., a_p}` is defined for each
  dimension of the position vector.
  At each iteration, element :math:`e` of the current position vector (:math:`e = a_i, i \in {1, ..., p}`) is updated
  by picking at random the left or right neighbor in the set of positions (e is set to :math:`a_{i-1}` or
  :math:`a_{i+1}`).)
- :class:`josiann.moves.discrete.SetStretch` (Modified version of the Stretch move with a defined set of possible
  positions (during the stretch process, the closest value in the set is chosen).)

Parallel moves :mod:`josiann.parallel.moves.discrete`
=====================================================

- :class:`josiann.parallel.moves.discrete.ParallelSetStep` (A version of :class:`josiann.moves.discrete.SetStep` suited
  for the parallel mode.)


.. _walkers:

Walkers
*******

A *walker* is a focus point of Josiann.
In regular settings, Josiann only cares about a single position vector so the number of walkers is 1.

With some special moves (ensemble moves) however, it is possible to obtain more information about the cost function to
optimize by evaluating it in multiple positions.
Each of the positions Josiann keeps track of is a distinct walker, each defined by its position vector and
associated cost.
At each iteration, the positions of all walkers are updated by the ensemble move, usually by taking into account the
position of other walkers.

Parameter `nb_walkers` allows the user to choose how many walkers to run.

.. note::

    When using :func:`josiann.vsa`, it is possible to vectorize on walkers (instead of vectorizing on the number of
    function evaluations):
    the cost for the position of all walkers will be computed at once. To do so, set parameter
    `vectorized_on_evaluations` to False.


Backup
******

When using discrete moves, the probability for a walker to reach the same position twice is sufficiently high to
cache past function evaluations to save later computation costs.
This is especially true with final iterations, when the walker does not move much.
When the backup is active, positions and associated costs are stored so that only :math:`n - l` evaluations need to be
computed instead of :math:`n`, where :math:`l` is the number of already computed function evaluations at the same
position.

To activate the backup, set the backup parameter to `True` when calling :func:`josiann.sa`, :func:`josiann.vsa`,
:func:`josiann.mcsa` or :func:`josiann.parallel.psa`.


Convergence detection
*********************

As the Temperature parameter drops, the fraction of accepted candidate moves drops significantly.
When a strong minimum exists, a large fraction of the last iterations are spent repeatedly evaluating the cost
functions at the same neighboring positions while.
To detect such cases of early convergence, Josiann can compute the Root-Mean-Square Deviation (RMSD) of the costs of
the last :math:`w` accepted positions.
When the RMSD drops below a threshold value, convergence is declared to have occurred and the algorithm stops early.

Convergence detection is activated by default but can be managed with the `detect_convergence` parameter.
The number :math:`w` of pasted accepted positions to consider and the threshold values can be configured through the
`window_size` and `tol` parameters respectively.


Result and Trace objects
************************

All 4 algorithms return a :class:`josiann.Result` object which contains :

- a :attr:`josiann.Result.success` boolean value to indicate if the optimization process was successful.
- a final :attr:`josiann.Result.message` string with details on the success.
- a :attr:`josiann.Result.trace` (:class:`josiann.Trace`) object which stores the coordinates of positions reached at
  each iteration and provides functions for plotting those positions.
- a :attr:`josiann.Result.parameters` object which stores run parameters.

.. warning::

    Plotting requires the Python library `plotly <https://plotly.com/python/>`_.


Parameters
**********

Common parameters are :

- `fun`: the cost function to optimize
- `x0`: a vector (or matrix) of initial positions
- `args`: additional arguments to be passed to `fun`. Those arguments are constant and will not be updated by the
  optimization algorithm.
- `bounds`: Optionally, min and max bounds for each dimension can be defined to limit the optimization search space.
- `moves`: one or more moves to update position vectors.
- `max_iter`: maximum number of iterations to compute before stopping the optimization algorithm.
- `max_measures`: maximum number of function evaluations to compute at a single position.
- `T_0`: initial value for the Temperature parameter.
- `detect_convergence`: whether to stop the optimization algorithm before reaching `max_iter` if convergence is detected.
- `tol`: tolerance for detecting convergence.
- `window_size`: number of iterations to consider for detecting convergence, computing the fraction of accepted moves
  and finding the best position reached (the position with the lowest associated cost).
- `seed`: an optional seed value to fix the randomness.


.. [1] Gutjahr, Walter J., and Georg Ch Pflug. "Simulated annealing for noisy cost functions." Journal of global
    optimization 8, no. 1 (1996): 1-13.

.. [2] Goodman, Jonathan, and Jonathan Weare. "Ensemble samplers with affine invariance." Communications in applied
    mathematics and computational science 5, no. 1 (2010): 65-80.
